= WebM muxer for WebCodecs

== Description

https://www.w3.org/TR/webcodecs/[WebCodecs] encodes audio and video but leaves multiplexing the
output into a container to the application. This project provides a way for your application
to get its audio and video tracks into a WebM container.

Under the hood it uses https://github.com/webmproject/libwebm/[libwebm] and
https://github.com/webmproject/webm-tools/[webmtools] compiled to Web Assembly using
https://emscripten.org/[Emscripten].

== Requirements

webm-muxer.js works on Chrome 93. The WebCodecs spec changes frequently so changes may
be required to maintain support going forward.

== Licence

webm-muxer.js is licensed under the terms of the link:LICENCE[MIT licence].

== Demo

You can see a demo https://rawgit-now.netlify.app/davedoesdev/webm-muxer.js/main/demo.html[here]
(tested on Chrome 94 Linux with `#enable-experimental-web-platform-features`).
The source code for the demo is available in link:demo.html[] and link:demo.js[].

When you click the *Start* button, you'll be asked by the browser to give permission to capture
your camera and microphone. The data from each is then passed to two separate workers which
encode the video into VP9 and audio into Opus using the WebCodecs browser API.

The encoded video and audio from each worker is passed into a third worker which muxes it into WebM format.

The WebM output from the third worker is then passed into a `<video>` element via
https://developer.mozilla.org/en-US/docs/Web/API/MediaSource[`MediaSource`] so you can see
it on the page.

When you click the **Stop** button, the camera and microphone are closed and the workers will exit
once they've processed the last of the data.

This all happens in near realtime &mdash; the data is effectively pipelined between the workers
and you don't have to wait until you press **Stop** to see the output.

You also have the option to record the WebM file and download it to your local disk.
If you check **Record** before clicking **Start**, the data will be buffered in memory
and then saved to `camera.webm` once you click **Stop**.

Finally, when you record, you can check **PCM** to have the raw audio data from your microphone
passed into the WebM muxer rather than encoding it to Opus. This option is only available when recording
because the `<video>` element doesn't support PCM playback from WebM files &mdash; you won't
be able to monitor the video in this case.

== Integration into your application

In your application, there are two Javascript files which you should run in Web Workers:

link:encoder-worker.js[]:: Takes output from https://w3c.github.io/mediacapture-transform/#track-processor[`MediaStreamTrackProcessor`] and encodes it using WebCodecs https://www.w3.org/TR/webcodecs/#videoencoder-interface[`VideoEncoder`] or https://www.w3.org/TR/webcodecs/#audioencoder-interface[`AudioEncoder`]. You should run this in up to two Workers, one for video and one for audio.

link:webm-worker.js[]:: Takes output from `encoder-worker.js` and muxes it into WebM container format.

You'll also need copy link:webm-muxer.js[] and link:webm-muxer.wasm[] to your application because `webm-worker.js` uses them.

Your application should in general follow the procedure described below.

. Create your https://www.w3.org/TR/mediacapture-streams/#mediastreamtrack[`MediaStreamTrack`]s (e.g. using https://www.w3.org/TR/mediacapture-streams/#dom-mediadevices-getusermedia[`getUserMedia()`].

. Create up to two https://w3c.github.io/mediacapture-transform/#track-processor[`MediaStreamTrackProcessor`]s, one for your video track and one for your audio track.

. Create a Web Worker using `webm-worker.js`.

. Create up to two Web Workers using `encoder-worker.js`, one for video and one for audio.

. When your application receives a message from the Worker running `webm-worker.js`:

* If the message's `type` property is `start-stream` then:

** Send a message to the Worker running `encoder-worker.js` for video with the following properties:
[horizontal]
`type`:: `"start"`
`readable`:: The `readable` property of the `MediaStreamTrackProcessor` for the video. You'll need to transfer this to the worker.
`key_frame_interval`:: How often to generate a key frame, in seconds. Use `0` for no key frames.
`config`:: The https://www.w3.org/TR/webcodecs/#dictdef-videoencoderconfig[`VideoEncoderConfig`] for encoding the video.

** Send a message to the Worker running `encoder-worker.js` for audio with the following properties:
[horizontal]
`type`:: `"start"`
`audio`:: `true`
`readable`:: The `readable` property of the `MediaStreamTrackProcessor` for the audio. You'll need to transfer this to the worker.
`config`:: The https://www.w3.org/TR/webcodecs/#dictdef-audioencoderconfig[`AudioEncoderConfig`] for encoding the audio.

* If the message's `type` property is `muxed-data` then:

** The message's `data` property contains the next chunk of the WebM output as an
https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/ArrayBuffer[`ArrayBuffer`],
which your application can use as it likes.

* If the message's `type` property is `error` then a muxing error occurred and the `detail` property contains the error description.

* If the message's `type` property is `exit` then the muxer has finished (all the tracks have finished,
the muxer has flushed its buffers and sent back all the muxed data).

. When your application receives a message from one of the Workers running `encoder-worker.js`:

* If the message's `type` property is `error` then an encoding error occurred and the `detail` property contains the error description.

* If the message's `type` property is `exit` then the encoder has finished (its track ended).

* Otherwise send the message onto the Worker running `webm-worker.js`. You should transfer the `data` property.

. Send a message to the Worker running `webm-worker.js` with the following properties:
[horizontal]
`type`:: `"start"`
`webm_metadata`:: An object with the following properties:
+
[horizontal]
`max_segment_duration`::: Desired length in nanoseconds of each WebM output chunk. Use a `BigInt` to specify this.
`video`::: If you have a video track, an object with the following properties:
+
[horizontal]
`width`:::: Width of the encoded video in pixels.
`height`:::: Height of the encoded video in pixels.
`frame_rate`:::: Number of frames per second in the video. This property is optional.
`codec_id`:::: WebM codec ID to describe the video encoding method, e.g. `"V_VP9"` or `"V_MPEG4/ISO/AVC"`. See the https://www.matroska.org/technical/codec_specs.html[codec mappings page] for more values.
`audio`::: If you have an audio track, an object with the following properties:
+
[horizontal]
`sample_rate`:::: Number of audio samples per second in the encoded audio.
`channels`:::: Number of channels in the encoded audio.
`bit_depth`:::: Number of bits in each sample. This property is usually used only for PCM encoded audio.
`codec_id`:::: WebM codec ID to describe the audio encoding method, e.g. `"A_OPUS"` or `"A_PCM/FLOAT/IEEE"`. See the https://www.matroska.org/technical/codec_specs.html[codec mappings page] for more values.

. To stop muxing cleanly, wait for exit messages from all the Workers running `encoder-worker.js` and then send a message to the Worker running `webm-worker.js` with the following property:
[horizontal]
`type`:: `"end"`

== Output

Per above, your application will receive chunked WebM output in multiple `type: "muxed-data"` messages from the Worker running `webm-worker`.

These are suitable for live streaming but if you concatenate them, for example to record them to a file, please be aware that the result
will not be seekable.

You can use link:EBML.js[] to make the concatenated WebM data seeekable. See link:demo.js#L94[the demo] for an example of how to do this. `EBML.js` was generated
by https://github.com/muaz-khan/RecordRTC/blob/master/libs/EBML.js[RecordRTC] from its original source at
https://github.com/legokichi/ts-ebml[ts-ebml].
